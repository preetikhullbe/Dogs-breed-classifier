{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18yHpbuAgg7J-x3_qZLYyRIvgRD3QtNMT",
      "authorship_tag": "ABX9TyO8lyqxd+CifmSDoLIubvyS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetikhullbe/Dogs-breed-classifier/blob/main/Dogs_breed_classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hH9Vm_z-hAjE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FlMcn88R6kkf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os"
      ],
      "metadata": {
        "id": "RPPUNqIVKUU8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio as iio"
      ],
      "metadata": {
        "id": "5ShrMhDPajff"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "4g2jx27UkL8_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "source": [
        "# prompt: print some images from the folder dir\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import imageio as iio\n",
        "from PIL import Image\n",
        "import os\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "76O4dOW5nXpV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required module\n",
        "from pathlib import Path\n",
        "\n",
        "# get the path/directory\n",
        "# get the path/directory\n",
        "base_dir = \"/content/drive/MyDrive/dataset\"\n",
        "\n",
        "# iterate over files in\n",
        "# that directory\n",
        "images = Path(base_dir).glob('*.png')\n",
        "for image in images:\n",
        "    print(image)"
      ],
      "metadata": {
        "id": "-xpLzYC8wFYu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the directory\n",
        "images_list = os.listdir(base_dir)\n",
        "\n",
        "# Filter out non-image files (optional)\n",
        "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
        "images_list = [img for img in images_list if img.lower().endswith(valid_extensions)]\n",
        "\n",
        "# Function to display images\n",
        "def display_images(images_list, base_dir, num_images=5):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    for i, img_name in enumerate(images_list[:num_images]):\n",
        "        img_path = os.path.join(base_dir, img_name)\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(img_name)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "24w-p8LeclRI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 images\n",
        "display_images(images_list, base_dir, num_images=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NkrIaK80gCre",
        "outputId": "f0adba85-d47d-4d90-e148-3f2d7338795f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKtt02-BiMvC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory paths\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Create the directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "# If you have multiple classes, create subdirectories for each class\n",
        "classes = ['Yorkshire_Terrier', 'Rottweiler','Poodle','Labrador_Retriever','Golden_Retriever',\n",
        "     'German_Shepherd','Dachshund','BullDog','Boxer','Beagle']\n",
        "     # Add your classes here\n",
        "for class_name in classes:\n",
        "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(validation_dir, class_name), exist_ok=True)\n",
        "\n",
        "print(\"Directories created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vInJw3MiLAW",
        "outputId": "a53f511b-d09e-4b35-cc0e-ffcde115054b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories created successfully!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "\n",
        "# Get the list of all images\n",
        "# Check the glob pattern and base_dir to ensure they are correct\n",
        "all_images = glob.glob(os.path.join(base_dir, '**/*.[jp][pn]g'), recursive=True)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_images, validation_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to copy images to the target directory\n",
        "def copy_images(image_list, target_dir):\n",
        "    for image_path in image_list:\n",
        "        class_name = os.path.basename(image_path).split('_')[0]  # Assuming class name is part of the file name\n",
        "        target_class_dir = os.path.join(target_dir, class_name)\n",
        "        # Check if the file already exists in the target directory\n",
        "        if not os.path.exists(os.path.join(target_class_dir, os.path.basename(image_path))):\n",
        "            shutil.copy(image_path, target_class_dir)  # Copy only if it doesn't exist\n",
        "\n",
        "# Copy training images\n",
        "copy_images(train_images, train_dir)\n",
        "\n",
        "# Copy validation images\n",
        "copy_images(validation_images, validation_dir)\n",
        "\n",
        "print(\"Data split and copied successfully!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgDqFWs4NAmB",
        "outputId": "f04b4490-a45b-4b1d-ff54-d1e424273559"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split and copied successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data Augmentation and Generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "tpnMk7C647Il"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
        ")"
      ],
      "metadata": {
        "id": "n1dLWdBh5mwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96bea166-9b85-4a1c-b6e0-5719dda54775"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 476 images belonging to 10 classes.\n",
            "Found 222 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Building the CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(classes), activation='softmax')  # Use 'softmax' for multi-class classification\n",
        "])\n"
      ],
      "metadata": {
        "id": "fQY6XbSo5vvA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # Use 'categorical_crossentropy' for multi-class classification\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "-GY24yz555Ng"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7vKmNyMf58gv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bab63d-17a1-48a4-b4b7-207ef09be91b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 74, 74, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 36, 36, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 17, 17, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 36992)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               18940416  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19038794 (72.63 MB)\n",
            "Trainable params: 19038794 (72.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "kT6yRnnE6FLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc92c50a-2da6-4e1b-dcba-20d6ec06d0c5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "14/14 [==============================] - 67s 5s/step - loss: 1.8159 - accuracy: 0.2005 - val_loss: 1.5845 - val_accuracy: 0.3177\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 1.6806 - accuracy: 0.2320 - val_loss: 1.5598 - val_accuracy: 0.2917\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.6317 - accuracy: 0.2770 - val_loss: 1.5279 - val_accuracy: 0.4010\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 1.6105 - accuracy: 0.3198 - val_loss: 1.5310 - val_accuracy: 0.2969\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.5645 - accuracy: 0.3063 - val_loss: 1.4241 - val_accuracy: 0.4271\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.5196 - accuracy: 0.3401 - val_loss: 1.4103 - val_accuracy: 0.3750\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 1.5249 - accuracy: 0.3559 - val_loss: 1.3648 - val_accuracy: 0.4740\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.5349 - accuracy: 0.2928 - val_loss: 1.3595 - val_accuracy: 0.4115\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 42s 3s/step - loss: 1.4581 - accuracy: 0.3919 - val_loss: 1.2607 - val_accuracy: 0.4688\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.4411 - accuracy: 0.4009 - val_loss: 1.2229 - val_accuracy: 0.4844\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 1.3969 - accuracy: 0.3941 - val_loss: 1.2633 - val_accuracy: 0.4688\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.4011 - accuracy: 0.3851 - val_loss: 1.1680 - val_accuracy: 0.5625\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.3191 - accuracy: 0.4617 - val_loss: 1.0906 - val_accuracy: 0.5833\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 35s 3s/step - loss: 1.3052 - accuracy: 0.4617 - val_loss: 1.0856 - val_accuracy: 0.5677\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 1.2694 - accuracy: 0.4527 - val_loss: 1.2086 - val_accuracy: 0.5312\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 1.3226 - accuracy: 0.4392 - val_loss: 1.0499 - val_accuracy: 0.6510\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 1.2318 - accuracy: 0.4820 - val_loss: 0.9713 - val_accuracy: 0.6250\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.2081 - accuracy: 0.4910 - val_loss: 0.9741 - val_accuracy: 0.6458\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 1.1784 - accuracy: 0.5270 - val_loss: 0.9268 - val_accuracy: 0.6979\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.1911 - accuracy: 0.5090 - val_loss: 0.9250 - val_accuracy: 0.7188\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 1.1773 - accuracy: 0.5090 - val_loss: 0.9977 - val_accuracy: 0.5990\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 1.1219 - accuracy: 0.5541 - val_loss: 0.9251 - val_accuracy: 0.6823\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 32s 2s/step - loss: 1.1376 - accuracy: 0.5180 - val_loss: 0.8477 - val_accuracy: 0.6979\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 40s 3s/step - loss: 1.1363 - accuracy: 0.5563 - val_loss: 0.8566 - val_accuracy: 0.6979\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 32s 2s/step - loss: 1.0934 - accuracy: 0.5698 - val_loss: 0.7721 - val_accuracy: 0.7344\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 1.1045 - accuracy: 0.5653 - val_loss: 0.7854 - val_accuracy: 0.7344\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 35s 2s/step - loss: 1.0209 - accuracy: 0.5811 - val_loss: 0.7803 - val_accuracy: 0.7083\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 0.9996 - accuracy: 0.6306 - val_loss: 0.6960 - val_accuracy: 0.7552\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 0.9954 - accuracy: 0.5946 - val_loss: 0.8184 - val_accuracy: 0.7031\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 0.9782 - accuracy: 0.6284 - val_loss: 0.6774 - val_accuracy: 0.7552\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.9776 - accuracy: 0.6374 - val_loss: 0.6404 - val_accuracy: 0.8385\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 0.9139 - accuracy: 0.6464 - val_loss: 0.6181 - val_accuracy: 0.8385\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 36s 3s/step - loss: 0.9753 - accuracy: 0.6261 - val_loss: 0.6333 - val_accuracy: 0.8958\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.9373 - accuracy: 0.6396 - val_loss: 0.6262 - val_accuracy: 0.8802\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.9048 - accuracy: 0.6847 - val_loss: 0.5634 - val_accuracy: 0.8333\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.8895 - accuracy: 0.6689 - val_loss: 0.5439 - val_accuracy: 0.8125\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 35s 3s/step - loss: 0.8816 - accuracy: 0.6734 - val_loss: 0.5820 - val_accuracy: 0.8021\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.8970 - accuracy: 0.6622 - val_loss: 0.5695 - val_accuracy: 0.8802\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 36s 2s/step - loss: 0.8095 - accuracy: 0.7185 - val_loss: 0.4640 - val_accuracy: 0.9271\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 38s 3s/step - loss: 0.7798 - accuracy: 0.6892 - val_loss: 0.5202 - val_accuracy: 0.8542\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.8701 - accuracy: 0.6599 - val_loss: 0.5129 - val_accuracy: 0.9010\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 34s 2s/step - loss: 0.8309 - accuracy: 0.6734 - val_loss: 0.5016 - val_accuracy: 0.8646\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.7902 - accuracy: 0.7005 - val_loss: 0.4929 - val_accuracy: 0.8906\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.7851 - accuracy: 0.7185 - val_loss: 0.4684 - val_accuracy: 0.8698\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 36s 3s/step - loss: 0.7193 - accuracy: 0.7410 - val_loss: 0.4625 - val_accuracy: 0.8958\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.8014 - accuracy: 0.7072 - val_loss: 0.4616 - val_accuracy: 0.8698\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.7366 - accuracy: 0.7297 - val_loss: 0.4219 - val_accuracy: 0.9219\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.7374 - accuracy: 0.7410 - val_loss: 0.4519 - val_accuracy: 0.8490\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.6690 - accuracy: 0.7793 - val_loss: 0.3805 - val_accuracy: 0.9427\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.6477 - accuracy: 0.7860 - val_loss: 0.4473 - val_accuracy: 0.9010\n"
          ]
        }
      ]
    }
  ]
}